# DEUTSCHE BAHN DATENQUALIT√ÑTS-BERICHT

**Analyst:** Sebastian  
**Datum:** 8. Dezember 2025  
**Datensatz:** Deutsche Bahn API - Oktober 2024  
**Datens√§tze gepr√ºft:** 1,984,484

---

## EXECUTIVE SUMMARY

Bei der Analyse von fast 2 Millionen Zugfahrten der Deutschen Bahn aus Oktober 2024 wurden **6 kritische Datenqualit√§tsprobleme** identifiziert, die **alle 5 Dimensionen** von Datenqualit√§t betreffen. Die Probleme reichen von fehlenden Werten √ºber logische Inkonsistenzen bis hin zu unm√∂glichen Werten.

**Kritischste Befunde:**
- 46,235 negative Versp√§tungen (physikalisch unm√∂glich)
- 884,459 fehlende Zeitstempel (44,57% der Daten)
- 25,220 stornierte Z√ºge mit Versp√§tung (logischer Widerspruch)

---

## PROBLEM 1: Fehlende Bahnhofsnamen

### KATEGORIE
**COMPLETENESS** (Vollst√§ndigkeit)

### BESCHREIBUNG
35,757 Datens√§tze haben keinen Bahnhofsnamen (station_name = NULL). Dies betrifft 1,80% aller Zugfahrten. Die Bahnhofsinformation ist kritisch f√ºr jede geografische Analyse und sollte immer vorhanden sein.

### BETROFFENE DATEN
- **Spalte:** `station_name`
- **Anzahl Zeilen betroffen:** 35,757 (1.80% der Daten)
- **Schweregrad:** HOCH

### BEWEIS (SQL)
```sql
SELECT
    COUNT(*) as total,
    COUNT(*) - COUNT(station_name) as missing_station,
    ROUND((COUNT(*) - COUNT(station_name)) * 100.0 / COUNT(*), 2) as prozent
FROM deutsche_bahn_data
```

**Ergebnis:** 35,757 fehlende Werte (1.80%)

### AUSWIRKUNG
- **Geografische Analysen** sind unvollst√§ndig
- **Top-Bahnhof-Rankings** sind verzerrt
- **Regional-Reports** haben Datenl√ºcken
- **P√ºnktlichkeits-Statistiken pro Station** sind ungenau

### FIX-STRATEGIE
1. **Kurzfristig:** Nutze `xml_station_name` als Fallback (0 missing values)
2. **Mittelfristig:** Erg√§nze fehlende Namen √ºber EVA-Nummer aus Referenztabelle
3. **Langfristig:** Validierung an der API-Schnittstelle implementieren (station_name = NOT NULL)

---

## PROBLEM 2: Negative Versp√§tungen

### KATEGORIE
**VALIDITY** (G√ºltigkeit)

### BESCHREIBUNG
46,235 Zugfahrten haben negative Versp√§tungswerte (delay_in_min < 0). Der extremste Wert liegt bei **-1,432 Minuten** (fast -24 Stunden). Negative Versp√§tungen bedeuten, dass ein Zug "fr√ºher als geplant" ankam, aber Werte von mehreren hundert Minuten sind unrealistisch und deuten auf Datenfehler hin.

### BETROFFENE DATEN
- **Spalte:** `delay_in_min`
- **Anzahl Zeilen betroffen:** 46,235 (2.33% der Daten)
- **Schweregrad:** KRITISCH
- **Wertebereich:** -1,432 min bis 849 min

### BEWEIS (SQL)
```sql
SELECT
    MIN(delay_in_min) as min_delay,
    COUNT(CASE WHEN delay_in_min < 0 THEN 1 END) as negative_delays,
    ROUND(COUNT(CASE WHEN delay_in_min < 0 THEN 1 END) * 100.0 / COUNT(*), 2) as prozent
FROM deutsche_bahn_data
```

**Ergebnis:** 
- Minimum: -1,432 Minuten
- Negative Werte: 46,235 (2.33%)

### AUSWIRKUNG
- **P√ºnktlichkeits-Statistiken** sind falsch
- **Durchschnittliche Versp√§tung** ist verzerrt
- **KPI-Berichte** f√ºr Management sind fehlerhaft
- **Presse-Mitteilungen** k√∂nnten auf falschen Daten basieren
- **Machine Learning Modelle** lernen falsche Muster

### FIX-STRATEGIE
1. **Sofort:** Alle Werte < -30 Minuten als Invalid markieren und aus Berechnungen ausschlie√üen
2. **Kurzfristig:** Datenbereinigung durchf√ºhren:
   - Werte < -30 min ‚Üí auf -30 min setzen (Winsorizing)
   - Werte < -1000 min ‚Üí wahrscheinlich Datenfehler ‚Üí auf 0 setzen
3. **Langfristig:** 
   - API-Validierung: delay_in_min BETWEEN -30 AND 1000
   - Automatische Quality Checks bei Daten-Import
   - Monitoring f√ºr Extremwerte

---

## PROBLEM 3: Extreme Versp√§tungen (>120 min)

### KATEGORIE
**ACCURACY** (Genauigkeit)

### BESCHREIBUNG
1,350 Zugfahrten haben Versp√§tungen von √ºber 120 Minuten (2 Stunden). Die extremste Versp√§tung liegt bei **849 Minuten** (14 Stunden). Von diesen haben 32 Z√ºge sogar √ºber 300 Minuten (5+ Stunden) Versp√§tung. Die Top 10 extremsten F√§lle sind fast alle "Bus SEVS4" ohne Stationsnamen.

### BETROFFENE DATEN
- **Spalte:** `delay_in_min`
- **Anzahl Zeilen betroffen:** 1,350 (0.07% der Daten)
- **Schweregrad:** MITTEL
- **Extreme F√§lle (>300 min):** 32

### BEWEIS (SQL)
```sql
SELECT
    COUNT(CASE WHEN delay_in_min > 120 THEN 1 END) as extreme_delays,
    COUNT(CASE WHEN delay_in_min > 300 THEN 1 END) as ultra_extreme,
    MAX(delay_in_min) as max_delay
FROM deutsche_bahn_data
```

**Top 3 Extreme:**
1. Bus SEVS4 | N/A | 849 min
2. Bus SEVS4 | N/A | 831 min  
3. Bus SEVS4 | N/A | 763 min

### AUSWIRKUNG
- Frage: Sind Z√ºge mit 14 Stunden Versp√§tung nicht **storniert**?
- Diese Werte verzerren **Durchschnitts-Berechnungen** massiv
- **Ausrei√üer-Analyse** zeigt 178,579 statistische Outliers (9% der Daten)
- Vermutung: Viele sollten als `is_canceled = True` markiert sein

### FIX-STRATEGIE
1. **Sofort:** Manuelle Review der 32 Ultra-Extreme (>300 min)
2. **Kurzfristig:** 
   - Business-Rule definieren: Ab wie viel Versp√§tung gilt ein Zug als storniert?
   - Vorschlag: >180 min = automatisch als canceled markieren
3. **Langfristig:**
   - Bessere Datenerfassung: Stornierungen richtig markieren
   - Separate Kategorie f√ºr "versp√§tet & sp√§ter storniert"

---

## PROBLEM 4: Stornierte Z√ºge mit Versp√§tung

### KATEGORIE
**CONSISTENCY** (Konsistenz)

### BESCHREIBUNG
25,220 Zugfahrten sind als storniert markiert (`is_canceled = True`), haben aber gleichzeitig eine Versp√§tung (`delay_in_min > 0`). Dies ist ein logischer Widerspruch: Ein stornierter Zug kann keine Versp√§tung haben, da er nie gefahren ist.

### BETROFFENE DATEN
- **Spalten:** `is_canceled`, `delay_in_min`
- **Anzahl Zeilen betroffen:** 25,220 (1.27% der Daten)
- **Schweregrad:** HOCH

### BEWEIS (SQL)
```sql
SELECT
    COUNT(*) as inconsistent_count,
    ROUND(COUNT(*) * 100.0 / (SELECT COUNT(*) FROM deutsche_bahn_data), 2) as prozent
FROM deutsche_bahn_data
WHERE is_canceled = True AND delay_in_min > 0
```

**Ergebnis:** 25,220 inkonsistente Datens√§tze (1.27%)

### AUSWIRKUNG
- **Logik-Fehler** in allen Analysen die beide Felder nutzen
- **Berichte** sind inkonsistent (storniert ODER versp√§tet?)
- **Kunden-Information** ist widerspr√ºchlich
- **Regelbasierte Systeme** k√∂nnen fehlerhafte Entscheidungen treffen

### FIX-STRATEGIE
1. **Sofort:** Business-Regel kl√§ren:
   - Option A: `is_canceled = True` ‚Üí `delay_in_min = 0` setzen
   - Option B: Neue Kategorie "versp√§tet & dann storniert"
2. **Kurzfristig:** Datenbereinigung nach gew√§hlter Regel
3. **Langfristig:**
   - Datenmodell √ºberarbeiten (zus√§tzliches Feld: cancellation_reason)
   - API-Validierung: IF is_canceled THEN delay_in_min = 0

---

## PROBLEM 5: Doppelte Ride IDs

### KATEGORIE
**UNIQUENESS** (Eindeutigkeit)

### BESCHREIBUNG
30,522 `train_line_ride_id` Werte kommen mehrfach vor. Eine Ride ID sollte eine eindeutige Zugfahrt identifizieren. Duplikate deuten darauf hin, dass entweder die ID-Vergabe fehlerhaft ist oder dass eine Fahrt mehrmals erfasst wurde.

### BETROFFENE DATEN
- **Spalte:** `train_line_ride_id`
- **Anzahl Duplikat-Gruppen:** 30,522
- **Schweregrad:** MITTEL

### BEWEIS (SQL)
```sql
SELECT
    train_line_ride_id,
    COUNT(*) as anzahl
FROM deutsche_bahn_data
WHERE train_line_ride_id IS NOT NULL
GROUP BY train_line_ride_id
HAVING COUNT(*) > 1
LIMIT 10
```

**Zus√§tzlicher Befund:** 36,989 exakte Duplikate (gleiche Station, Zeit, Zug, Versp√§tung)

### AUSWIRKUNG
- **Fahrt-basierte Analysen** sind fehlerhaft
- **Doppelz√§hlungen** bei Aggregationen
- **Joining** mit anderen Tabellen √ºber ride_id liefert falsche Ergebnisse
- **Tracking** von einzelnen Z√ºgen unm√∂glich

### FIX-STRATEGIE
1. **Sofort:** Analyse warum Duplikate entstehen:
   - Mehrfache API-Calls f√ºr gleiche Fahrt?
   - Updates die als neue Zeilen gespeichert werden?
2. **Kurzfristig:** 
   - Deduplication: Nur neueste Version pro ride_id behalten
   - Oder: GROUP BY ride_id mit Aggregation
3. **Langfristig:**
   - Prim√§rschl√ºssel auf ride_id + timestamp
   - Upsert-Logik statt Insert-Only

---

## PROBLEM 6: Fehlende Zeitstempel

### KATEGORIE
**COMPLETENESS** (Vollst√§ndigkeit)

### BESCHREIBUNG
√úber 44% der Datens√§tze haben fehlende Zeitstempel f√ºr Ankunft oder Abfahrt. Dies ist das gr√∂√üte Datenqualit√§tsproblem nach Anzahl betroffener Zeilen.

**Fehlende Werte:**
- `arrival_planned_time`: 441,997 (22.27%)
- `arrival_change_time`: 441,914 (22.27%)
- `departure_planned_time`: 442,462 (22.30%)
- `departure_change_time`: 442,377 (22.29%)

### BETROFFENE DATEN
- **Spalten:** Alle Zeit-Spalten
- **Anzahl Zeilen betroffen:** ~442,000 pro Spalte (Gesamt: 884,459 unique Zeilen)
- **Schweregrad:** HOCH
- **Prozent:** 44.57% der Daten

### BEWEIS (SQL)
```sql
SELECT
    COUNT(*) - COUNT(arrival_planned_time) as missing_arr_planned,
    COUNT(*) - COUNT(departure_planned_time) as missing_dep_planned,
    ROUND((COUNT(*) - COUNT(arrival_planned_time)) * 100.0 / COUNT(*), 2) as prozent
FROM deutsche_bahn_data
```

### AUSWIRKUNG
- **Zeit-basierte Analysen** funktionieren nur f√ºr 55% der Daten
- **P√ºnktlichkeits-Berechnungen** sind eingeschr√§nkt
- **Delay-Berechnungen** k√∂nnen nicht validiert werden
- **Durchschnittliche Versp√§tung** kann nicht nachgerechnet werden

### FIX-STRATEGIE
1. **Sofort:** Verstehen warum Zeitstempel fehlen:
   - Sind das Durchfahrten (keine Ankunft/Abfahrt)?
   - Sind das Endhaltestellen (keine Abfahrt) oder Starthaltestellen (keine Ankunft)?
2. **Kurzfristig:**
   - Dokumentieren welche Datens√§tze erwartbare NULLs haben
   - Separate Analyse f√ºr "vollst√§ndige" Datens√§tze
3. **Langfristig:**
   - Datenmodell klarer definieren
   - Flag-Feld: `is_intermediate_stop`, `is_final_station`, etc.

---

## WEITERE BEFUNDE

### Zeitliche Paradoxe
**928,020 Datens√§tze** (46.76%) haben `arrival_planned_time < departure_planned_time`. Dies ist bei Durchfahrten normal (Ankunft, dann Abfahrt), aber die hohe Zahl sollte validiert werden.

### Encoding
**25 Bahnh√∂fe** haben Umlaute in den Namen (√§, √∂, √º, √ü). Dies ist korrekt f√ºr deutsche St√§dte (M√ºnchen, D√ºsseldorf, etc.), aber Encoding sollte konsistent UTF-8 sein.

### Statistische Outliers
Nach der **IQR-Methode** sind **178,579 Werte** (9%) statistische Ausrei√üer bei den Versp√§tungen. Das ist sehr hoch und deutet auf Datenqualit√§tsprobleme hin.

---

## VERTEILUNGS-ERKENNTNISSE

### Top-Problematische Bahnh√∂fe
Nach durchschnittlicher Versp√§tung:
1. **K√∂ln Hbf:** 6.33 min avg delay (32,098 Fahrten)
2. **M√ºnchen Hbf:** 5.99 min avg delay (64,663 Fahrten)
3. **D√ºsseldorf Hbf:** 5.86 min avg delay (43,367 Fahrten)

### Top-Problematische Zugtypen
1. **ICE:** 10.29 min avg delay, 6.92% canceled
2. **IC:** 9.33 min avg delay, 9.34% canceled (h√∂chste Ausfallrate!)
3. **ME:** 5.34 min avg delay

### Zeitliche Muster
- **H√∂chste Versp√§tungen:** Donnerstag (3.99 min avg)
- **Niedrigste Versp√§tungen:** Sonntag (3.07 min avg)
- **Meiste Ausf√§lle:** Freitag (17,962 canceled)

---

## EMPFEHLUNGEN

### 1. SOFORT (Diese Woche)
‚úÖ **Negative Versp√§tungen korrigieren**
- Alle Werte < -1000 min ‚Üí Datenfehler ‚Üí Flag setzen
- Business-Regel definieren f√ºr acceptable negative delays

‚úÖ **Storniert + Versp√§tung Logik kl√§ren**
- Meeting mit Daten-Besitzern: Was bedeutet das?
- Entscheidung: Bereinigungsregel festlegen

### 2. KURZFRISTIG (N√§chste 2 Wochen)
üìä **Datenbereinigung durchf√ºhren**
- Skript schreiben f√ºr alle 6 Probleme
- Bereinigte Version als `data-2024-10-cleaned.parquet` exportieren

üìã **Dokumentation erstellen**
- Data Dictionary mit erlaubten Wertebereichen
- Business Rules dokumentieren

### 3. LANGFRISTIG (N√§chste 3 Monate)
üîÑ **Automatisierung**
- CI/CD Pipeline mit Data Quality Checks
- Automatische Validierung bei jedem Daten-Import
- Alerting wenn Schwellwerte √ºberschritten

üéØ **Datenmodell verbessern**
- Zus√§tzliche Felder f√ºr bessere Semantik
- Klare Definition was NULL bedeutet
- Constraints an der Datenquelle

---

## TECHNISCHE DETAILS

### Verwendete Tools
- **DuckDB** f√ºr SQL-Analysen
- **Pandas** f√ºr Daten-Exploration
- **Python** f√ºr Automatisierung

### Analysierte Zeitraum
Oktober 2024 (01.10.2024 - 31.10.2024)

### Datensatz
- **Quelle:** Deutsche Bahn API (via HuggingFace)
- **Format:** Parquet
- **Gr√∂√üe:** 72 MB
- **Zeilen:** 1,984,484

### Methodik
Alle 5 Dimensionen von Datenqualit√§t wurden systematisch gepr√ºft:
1. ‚úÖ COMPLETENESS (Vollst√§ndigkeit)
2. ‚úÖ VALIDITY (G√ºltigkeit)
3. ‚úÖ CONSISTENCY (Konsistenz)
4. ‚úÖ ACCURACY (Genauigkeit)
5. ‚úÖ UNIQUENESS (Eindeutigkeit)

---

## FAZIT

Die Analyse zeigt **erhebliche Datenqualit√§tsprobleme** in allen 5 Dimensionen. Mit **6 kritischen Problemen** die √ºber **1 Million Datens√§tze** betreffen, ist eine **sofortige Bereinigung notwendig** bevor die Daten f√ºr Business-Entscheidungen oder Machine Learning verwendet werden.

**Business Impact:** 
- Ohne Bereinigung sind P√ºnktlichkeits-Reports **unzuverl√§ssig**
- KPIs k√∂nnen **nicht korrekt berechnet** werden
- Presse-Mitteilungen k√∂nnten auf **falschen Zahlen** basieren
- ML-Modelle w√ºrden **falsche Muster** lernen

**N√§chster Schritt:** Meeting mit Data-Besitzern zur Kl√§rung der Business-Regeln und Start der Datenbereinigung.

---

**Ende des Berichts**

*Erstellt am: 8. Dezember 2025*  
*Analysedauer: ~45 Minuten*  
*Analyst: Sebastian*

